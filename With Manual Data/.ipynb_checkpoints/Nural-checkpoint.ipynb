{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=.8*1200\n",
    "k=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_train_data = \"./data/2018_4_21_22_38_train.csv\"\n",
    "ccf_test_data = \"./data/2018_4_30_08_33_test.csv\"\n",
    "\n",
    "\n",
    "def load_data(filepath):\n",
    "    from numpy import genfromtxt\n",
    "\n",
    "    csv_data = genfromtxt(filepath, delimiter=\",\", skip_header=1)\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for d in csv_data:\n",
    "        data.append(d[:-1])\n",
    "        labels.append(d[-1])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11934 37471]\n",
      " [ 9078 41517]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.24      0.34     49405\n",
      "        1.0       0.53      0.82      0.64     50595\n",
      "\n",
      "avg / total       0.55      0.53      0.49    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    train_dataset, train_labels = load_data(ccf_train_data)\n",
    "    test_dataset, test_labels = load_data(ccf_test_data)\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(17,17,17,17,17,17,17), max_iter=500)\n",
    "    mlp.fit(train_dataset,train_labels)\n",
    "    \n",
    "    predictions = mlp.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11934 37471]\n",
      " [ 9078 41517]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.24      0.34     49405\n",
      "        1.0       0.53      0.82      0.64     50595\n",
      "\n",
      "avg / total       0.55      0.53      0.49    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = 0;\n",
    "for i in range(0,len(predictions)):\n",
    "    if predictions[i] != test_labels[i]:\n",
    "        missed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53451\n"
     ]
    }
   ],
   "source": [
    "print(1 - (missed/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
